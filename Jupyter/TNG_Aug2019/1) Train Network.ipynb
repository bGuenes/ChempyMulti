{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/home/oliverphilcox/ChempyMulti/')\n",
    "from Chempy.parameter import ModelParameters\n",
    "a=ModelParameters()\n",
    "from Chempy.cem_function import single_timestep_chempy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 915956 training data points for a 7->8 shape network\n"
     ]
    }
   ],
   "source": [
    "# Define elements to use:\n",
    "\n",
    "els = ['C','Fe','He','Mg','N','Ne','O','Si'] # TNG elements\n",
    "\n",
    "# Load training data:\n",
    "full_input = '/mnt/store1/oliverphilcox/ChempyMultiData/TNG/Random_Training_Data_TNG_500000_0_v2.npz'\n",
    "dat1=np.load(full_input,mmap_mode='r')\n",
    "full_input2 = '/mnt/store1/oliverphilcox/ChempyMultiData/TNG/Random_Training_Data_TNG_500000_1_v2.npz'\n",
    "dat2=np.load(full_input2,mmap_mode='r')\n",
    "\n",
    "\n",
    "all_els = dat1['elements']\n",
    "for e in range(len(all_els)):\n",
    "    assert dat1['elements'][e]==dat2['elements'][e]\n",
    "    \n",
    "#params = dat1['params']#[:100000]\n",
    "params=np.concatenate([dat1['params'],dat2['params']])\n",
    "#big_abun = dat1.f.abundances#[:100000]\n",
    "big_abun=np.concatenate([dat1.f.abundances,dat2.f.abundances])\n",
    "\n",
    "el_indices=np.zeros(len(els),dtype=int)\n",
    "for e,el in enumerate(els):\n",
    "    el_indices[e]=np.where(el==all_els)[0]\n",
    "    \n",
    "# Filter out unwanted elements\n",
    "abun = big_abun[:,el_indices]\n",
    "\n",
    "\n",
    "## Remove any bad runs\n",
    "bitmap=np.ones(len(params),dtype=int)\n",
    "for i,ab in enumerate(abun):\n",
    "    if ab[0]==0:\n",
    "        bitmap[i]=0\n",
    "\n",
    "cut_params=params[np.where(bitmap==1)]\n",
    "cut_abuns=abun[np.where(bitmap==1)]\n",
    "\n",
    "good_index=np.where(np.isfinite(cut_abuns).all(axis=1))[0] # remove infinities\n",
    "cut_params2=cut_params[good_index]\n",
    "cut_abuns2=cut_abuns[good_index]\n",
    "good_index2=np.where(cut_params2[:,-1]>0.99)[0] # remove bad birth times\n",
    "cut_params3=cut_params2[good_index2]\n",
    "cut_abuns3=cut_abuns2[good_index2]\n",
    "\n",
    "# Set standardization parameters\n",
    "par_mean=np.mean(cut_params3,axis=0)\n",
    "par_std=np.std(cut_params3,axis=0)\n",
    "ab_mean=np.mean(cut_abuns3,axis=0)\n",
    "ab_std=np.std(cut_abuns3,axis=0)\n",
    "\n",
    "# Change birth-time mean/std to give t in [0,1] - we use (T-mean_T)/std_T here so this works\n",
    "par_mean[-1]=min(cut_params3[:,-1])\n",
    "par_std[-1]=(max(cut_params3[:,-1])-min(cut_params3[:,-1]))\n",
    "\n",
    "# Now randomize the selection\n",
    "len_filt=np.random.choice(range(len(cut_abuns3)),replace=False,size=len(cut_abuns3))\n",
    "\n",
    "# Create randomized and standardized training data\n",
    "trainX=(cut_params3[len_filt]-par_mean)/par_std\n",
    "trainY=(cut_abuns3[len_filt]-ab_mean)/ab_std\n",
    "\n",
    "# Add in T^2 term for accuracy\n",
    "n_poly=2\n",
    "sq_trainX=np.zeros([trainX.shape[0],trainX.shape[1]+n_poly-1])#+2])\n",
    "sq_trainX[:,:trainX.shape[1]]=trainX\n",
    "for i in range(n_poly-1):\n",
    "    sq_trainX[:,trainX.shape[1]+i]=trainX[:,-1]**(i+2)\n",
    "\n",
    "print('Using %d training data points for a %d->%d shape network'%(sq_trainX.shape[0],sq_trainX.shape[1],trainY.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 45723 test data points\n"
     ]
    }
   ],
   "source": [
    "# Load test data:\n",
    "datT=np.load('/mnt/store1/oliverphilcox/ChempyMultiData/TNG/Random_Test_Data_TNG_50000_0_v2.npz')\n",
    "abunT=datT.f.abundances[:,el_indices]\n",
    "elsT=datT.f.elements\n",
    "paramsT=datT.f.params\n",
    "\n",
    "bitmapT=np.ones(len(paramsT),dtype=int)\n",
    "for i,ab in enumerate(abunT):\n",
    "    if ab[0]==0:\n",
    "        bitmapT[i]=0\n",
    "                \n",
    "# Remove dodgy data\n",
    "cut_paramsT=paramsT[np.where(bitmapT==1)]\n",
    "cut_abunsT=abunT[np.where(bitmapT==1)]\n",
    "good_indexT=np.where((np.isfinite(cut_abunsT).all(axis=1)))\n",
    "cut_params2T=cut_paramsT[good_indexT]\n",
    "cut_abuns2T = cut_abunsT[good_indexT]\n",
    "good_index2T=np.where(cut_params2T[:,-1]>0.99)\n",
    "cut_params3T=cut_params2T[good_index2T]\n",
    "cut_abuns3T = cut_abuns2T[good_index2T]\n",
    "                     \n",
    "# Standardize using same standardizations as before\n",
    "testX=(cut_params3T-par_mean)/par_std\n",
    "testY=(cut_abuns3T-ab_mean)/ab_std\n",
    "\n",
    "sq_testX=np.zeros([testX.shape[0],testX.shape[1]+n_poly-1])#+2])\n",
    "sq_testX[:,:testX.shape[1]]=testX\n",
    "for i in range(n_poly-1):\n",
    "    sq_testX[:,testX.shape[1]+i]=testX[:,-1]**(i+2)\n",
    "\n",
    "print(\"Using %d test data points\"%sq_testX.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the network in scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import cross_validation\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "def single_regressor(neurons,el_index,epochs=1000,verbose=True):\n",
    "    \"\"\"Return out-of-sample score for a given number of neurons for one element\"\"\"\n",
    "    model=MLPRegressor(solver='adam',alpha=0.001,max_iter=epochs,learning_rate='adaptive',tol=1e-13,\n",
    "                       hidden_layer_sizes=(neurons,),activation='tanh',verbose=verbose,\n",
    "                      shuffle=True,early_stopping=True)#,learning_rate_init=0.1)\n",
    "\n",
    "    model.fit(sq_trainX,trainY[:,el_index])\n",
    "\n",
    "    model_pred=model.predict(sq_testX)\n",
    "    score = np.mean((model_pred-testY[:,el_index])**2.)\n",
    "    diff = np.abs(testY[:,el_index]-model_pred)\n",
    "    w0,w1=model.coefs_\n",
    "    b0,b1=model.intercepts_\n",
    "    return score,diff,[w0,w1,b0,b1]\n",
    "\n",
    "def all_regressor(neurons,epochs=1000,verbose=True):\n",
    "    \"\"\"Return out-of-sample score for a given number of neurons for all elements\"\"\"\n",
    "    model=MLPRegressor(solver='adam',alpha=0.001,max_iter=epochs,learning_rate='adaptive',tol=1e-13,\n",
    "                       hidden_layer_sizes=(neurons,),activation='tanh',verbose=verbose,\n",
    "                      shuffle=True,early_stopping=True)#,learning_rate_init=0.1)\n",
    "\n",
    "    model.fit(sq_trainX,trainY)\n",
    "\n",
    "    model_pred=model.predict(sq_testX)\n",
    "    scores = np.mean((model_pred-testY)**2.,axis=0)\n",
    "    diffs = np.abs(testY-model_pred)\n",
    "    w0,w1=model.coefs_\n",
    "    b0,b1=model.intercepts_\n",
    "    return scores,diffs,[w0,w1,b0,b1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run single neuron nets for various $n_\\mathrm{neuron}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def neural_run(nn):\n",
    "    print(\"Running for %d neurons\"%nn)\n",
    "    return single_regressor(nn,0)[0]\n",
    "import multiprocessing as mp\n",
    "import tqdm\n",
    "p=mp.Pool()\n",
    "all_neurons = np.arange(5,85,5)\n",
    "neuron_scores=list(tqdm.tqdm(p.imap(neural_run,all_neurons),total=len(all_neurons)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez('8element_net_variable_neurons_L2_scores',neurons=all_neurons,scores=neuron_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d=np.load('8element_net_variable_neurons_L2_scores.npz')\n",
    "all_neurons=d['neurons']\n",
    "neuron_scores=d['scores']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(all_neurons,neuron_scores);plt.ylabel('L2 Score',fontsize=14);\n",
    "plt.xlabel(r'$n_\\mathrm{neuron}$',fontsize=14);\n",
    "plt.yscale('log')\n",
    "plt.ylim([4e-4,1e-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now train networks in parallel using $n_\\mathrm{neuron}=40$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neurons=40\n",
    "def mp_run(el_i):\n",
    "    print(\"Running net %d of %d\"%(int(el_i)+1,len(els)))\n",
    "    output = single_regressor(neurons,int(el_i),epochs=3000)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "p=mp.Pool()\n",
    "import tqdm\n",
    "output=list(tqdm.tqdm(p.imap(mp_run,range(len(els))),total=len(els)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_scores = np.zeros(len(els))\n",
    "all_diffs = np.zeros([len(els),len(testY)])\n",
    "coeffs=[]\n",
    "for el_i in range(len(els)):\n",
    "    all_scores[el_i],all_diffs[el_i],co=output[el_i]\n",
    "    coeffs.append(co)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w0=np.hstack([co[0] for co in coeffs])\n",
    "b0=np.hstack([co[2] for co in coeffs])\n",
    "b1=np.hstack([co[3] for co in coeffs])\n",
    "\n",
    "## Read in w1 vector into sparse structure\n",
    "w1=np.zeros([w0.shape[1],b1.shape[0]])\n",
    "assert neurons==w0.shape[1]/len(coeffs)\n",
    "for i in range(len(coeffs)):\n",
    "    w1[int(neurons*i):int(neurons*(i+1)),i]=coeffs[i][1][:,0]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stacked_net_output(in_par):\n",
    "    l1=np.matmul(in_par,w0)+b0\n",
    "    return np.matmul(np.tanh(l1),w1)+b1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save output\n",
    "np.savez('/mnt/store1/oliverphilcox/ChempyMultiData/TNG/stacked_8_element_net.npz',w0=w0,w1=w1,b0=b0,b1=b1,\n",
    "         in_mean=par_mean,in_std=par_std,out_mean=ab_mean,out_std=ab_std,\n",
    "         activation='tanh',neurons=neurons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload data\n",
    "dat=np.load('/mnt/store1/oliverphilcox/ChempyMultiData/TNG/stacked_8_element_net.npz')\n",
    "w0=dat['w0'];w1=dat['w1'];b0=dat['b0'];b1=dat['b1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1_err = np.abs(stacked_net_output(sq_testX)-testY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(l1_err[:,2],range=[0,.4],bins=100);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run for all networks together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "comb_scores,comb_diffs,_ = all_regressor(40,epochs=3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.errorbar(1,np.mean(all_scores),yerr=np.std(all_scores),marker='x',label='Stacked Nets')\n",
    "plt.errorbar(2,np.mean(comb_scores),yerr=np.std(comb_scores),marker='x',label='Combined Net')\n",
    "plt.legend();plt.title('MSE Scores');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(all_diffs.ravel(),range=[0,np.percentile(all_diffs.ravel(),99)],alpha=0.5,bins=50,label='Stacked Single Nets');\n",
    "plt.hist(comb_diffs.ravel(),range=[0,np.percentile(all_diffs.ravel(),99)],alpha=0.5,bins=50,label='Combined Nets');\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compute L1 differences for combined and stacked nets (with correct normalizations)\n",
    "real_all_diffs = np.asarray([ad*ab_std for ad in all_diffs.T])\n",
    "real_comb_diffs = np.asarray([cd*ab_std for cd in comb_diffs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez('/mnt/store1/oliverphilcox/ChempyMultiData/TNG/Training_plot_data.npz',\n",
    "        real_all_diffs=real_all_diffs,\n",
    "        real_comb_diffs=real_comb_diffs,\n",
    "        els=els)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def percs(data,axis=0):\n",
    "    percs=np.percentile(data,[15.865,50.,100.-15.865],axis=axis)\n",
    "    return percs[1],percs[1]-percs[0],percs[2]-percs[1]\n",
    "all_percs=percs(real_all_diffs)\n",
    "comb_percs=percs(real_comb_diffs)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.errorbar(np.arange(len(els))-0.05,all_percs[0],\n",
    "             yerr=[all_percs[1],all_percs[2]],label='Parallel Nets',c='b');\n",
    "plt.errorbar(np.arange(len(els))+0.05,comb_percs[0],\n",
    "             yerr=[comb_percs[1],comb_percs[2]],label='Combined Net',c='r');FS=18\n",
    "plt.legend(fontsize=14);plt.ylabel('L1 Distance [dex]',fontsize=FS);plt.xlabel('Abundance',fontsize=FS)\n",
    "\n",
    "names = []\n",
    "for el in els:\n",
    "    if el!='Fe':\n",
    "        names.append('[%s/Fe]'%el)\n",
    "    else:\n",
    "        names.append('[Fe/H]')\n",
    "\n",
    "plt.xticks(range(8),names,fontsize=16);\n",
    "plt.savefig('Plots_New/L1_Element_Error.pdf',bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1,p2,p3=np.percentile(real_all_diffs,[15.865,50.,100-17.865])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(r'$%.3f_{-%.3f}^{+%.3f}$'%(p2,p2-p1,p3-p2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Single Stacked Nets\",np.mean(real_all_diffs).round(3),np.std(real_all_diffs).round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Combined Nets\",np.mean(real_comb_diffs).round(3),np.std(real_comb_diffs).round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So the single nets seem to perform better here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now access the convergence across parameter space:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using mean L1 error here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_par = cut_params3\n",
    "test_par=cut_params3T\n",
    "stacked_pred =stacked_net_output(sq_testX)\n",
    "stacked_diff = np.mean(np.abs((stacked_pred-testY)*ab_std),axis=1) # destandardize here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tr = train_par\n",
    "data_v = test_par\n",
    "param_error = stacked_diff\n",
    "\n",
    "# Initialize plot\n",
    "plt.clf()\n",
    "text_size = 12\n",
    "\n",
    "#plt.rc('text', usetex=False)\n",
    "#plt.rc('font', family='sans-serif')\n",
    "\n",
    "#plt.rc('font', family='serif',size = text_size)\n",
    "#plt.rc('xtick', labelsize=text_size)\n",
    "#plt.rc('ytick', labelsize=text_size)\n",
    "#plt.rc('axes', labelsize=text_size, lw=1.0)\n",
    "#plt.rc('lines', linewidth = 1)\n",
    "#plt.rcParams['ytick.major.pad']='8'\n",
    "#plt.rcParams['text.latex.preamble']=[r\"\\usepackage{libertine}\"]\n",
    "#params = {'text.usetex' : True,\n",
    "#      'font.family' : 'libertine',\n",
    "#      'text.latex.unicode': True,\n",
    "#      }\n",
    "#plt.rcParams.update(params)\n",
    "parameter_names = [r'$\\alpha_\\mathrm{IMF}$',r'$\\log_{10}(\\mathrm{N_{Ia}})$',\n",
    "               r'$\\log_{10}(\\mathrm{SFE})$',\n",
    "               r'$\\log_{10}(\\mathrm{SFR_{peak}})$',r'$\\mathrm{x}_\\mathrm{out}$',r'$T_\\mathrm{star}$']\n",
    "\n",
    "\n",
    "# Plot settings\n",
    "fig,axes = plt.subplots(nrows = 6, ncols = 6,figsize=(14.69,8.0))#,dpi=300)\n",
    "alpha = 0.5\n",
    "lw=2 # Linewidth\n",
    "left = 0.1 # Left side of subplots\n",
    "right = 0.8 # Right side\n",
    "bottom = 0.075\n",
    "top = 0.97\n",
    "wspace = 0.0 # blankspace width between subplots\n",
    "hspace = 0.0 # blankspace height between subplots\n",
    "color_max = np.percentile(param_error,99.)#0.05#a.color_max\n",
    "plt.subplots_adjust(left=left,bottom=bottom,right=right,top=top,wspace=wspace,hspace=hspace)\n",
    "\n",
    "cmap= cm.YlGnBu\n",
    "\n",
    "# Create plot\n",
    "for i in range(6):\n",
    "    for j in range(6):\n",
    "        axes[i,j].locator_params(nbins=4)\n",
    "        if j==1:\n",
    "            axes[i,j].locator_params(nbins=4)\n",
    "        if i==j:\n",
    "            counts,edges = np.histogram(np.asarray(data_v[:,j]),bins=10)\n",
    "            max_count = float(np.max(counts))\n",
    "            counts = np.divide(counts,max_count)\n",
    "            median = np.zeros(len(edges)-1)\n",
    "            for k in range(len(edges)-1):\n",
    "                choice = np.logical_and(np.greater(data_v[:,j],edges[k]),np.less(data_v[:,j],edges[k+1]))\n",
    "                error=np.extract(choice,param_error)\n",
    "                if len(error) != 0:\n",
    "                    median[k] = np.median(error)\n",
    "            colors = cmap(median/color_max)\n",
    "            axes[i,j].bar(x = edges[:-1], height=counts, width = edges[1]-edges[0],\n",
    "                                color=colors,alpha=alpha, linewidth=0,rasterized=True)\n",
    "            axes[i,j].set_xlim(min(data_v[:,j]),max(data_v[:,j]))\n",
    "            axes[i,j].set_ylim(0,1.05)\n",
    "            if j !=0:\n",
    "                plt.setp(axes[i,j].get_yticklabels(), visible=False)\n",
    "            axes[i,j].vlines(np.percentile(data_v[:,j],15.865),axes[i,j].get_ylim()[0],axes[i,j].get_ylim()[1], color = 'k',alpha=alpha,linewidth = lw,linestyle = 'dashed')\n",
    "            axes[i,j].vlines(np.percentile(data_v[:,j],100-15.865),axes[i,j].get_ylim()[0],axes[i,j].get_ylim()[1], color = 'k',alpha=alpha,linewidth = lw,linestyle = 'dashed')\n",
    "            axes[i,j].vlines(np.percentile(data_v[:,j],50),axes[i,j].get_ylim()[0],axes[i,j].get_ylim()[1], color = 'k',alpha=alpha,linewidth = lw)\n",
    "        if i>j:\n",
    "            if j !=0:\n",
    "                plt.setp(axes[i,j].get_yticklabels(), visible=False)\n",
    "            P1 = axes[i,j].scatter(data_v[:,j],data_v[:,i],marker='x',alpha=0.3,\n",
    "                                    c=param_error,vmin=0,vmax=color_max,cmap=cmap,s=3,rasterized=True)\n",
    "            #P2 = axes[i,j].scatter(data_tr[:,j],data_tr[:,i],c='k',marker='+',s=80)\n",
    "            axes[i,j].set_xlim(min(data_tr[:,j]),max(data_tr[:,j]))\n",
    "            axes[i,j].set_ylim(min(data_tr[:,i]),max(data_tr[:,i]))\n",
    "        if j>i:\n",
    "            axes[i,j].axis('off')\n",
    "        if i == 5:\n",
    "            axes[i,j].set_xlabel(parameter_names[j])\n",
    "        if j ==0:\n",
    "            axes[i,j].set_ylabel(parameter_names[i])\n",
    "        if i==2 and j == 1:\n",
    "            cplot = axes[i,j].scatter(data_v[:,j],data_v[:,i],marker='.',alpha=0.3,\n",
    "                                                c=param_error,vmin=0,vmax=color_max,\n",
    "                                                cmap=cmap,s=3,rasterized=True)\n",
    "            axes[i,j].set_xlim(min(data_tr[:,j]),max(data_tr[:,j]))\n",
    "            axes[i,j].set_ylim(min(data_tr[:,i]),max(data_tr[:,i]))\n",
    "cax=fig.add_axes([0.82,0.06,0.02,0.9]);\n",
    "plt.colorbar(cplot,cax=cax);\n",
    "\n",
    "#plt.savefig('Plots/Network_Error_Param_Space.pdf',bbox_inches='tight')\n",
    "\n",
    "plt.show();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig('Plots_New/Network_Error_Param_Space.png',dpi=300)#,bbox_inches='tight')#,dpi=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
